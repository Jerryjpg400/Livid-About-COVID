{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# Data manipulation and file loading\n",
    "import pandas\n",
    "import numpy as np\n",
    "import math\n",
    "#Keras model and layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,LSTM,Concatenate,Layer,Lambda,Input,Multiply,SimpleRNN,GRU,TimeDistributed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras import optimizers\n",
    "import keras.backend\n",
    "import tensorflow as tf\n",
    "from keras import initializers      \n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data Formatting **\n",
    "Here, we convert the data to sequences with a \"look-back\" parameter. This simply creates sequences of such a size that are fed to the model to learn to predict the next value. The look-back is a parameter with the value being 7 days in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert time-series data to LSTM training set\n",
    "\n",
    "# The sequence over the entire dataset and one-step ahead value for prediction\n",
    "\n",
    "# Perform a one-step ahead prediction over the entire sequence or look-back sized sequence \n",
    "def create_dataset(mode, ns, fname, look_back=1)\n",
    "    dataX, dataY = [], []\n",
    "    maxD = 0\n",
    "    minD = 10000\n",
    "    for c in ns:\n",
    "        dataframe = pandas.read_csv(fname, usecols=[int(c)], engine='python')\n",
    "        dataframe.dropna(how='all',inplace=True)\n",
    "        dataset = dataframe.values\n",
    "        dataset = dataset.astype('float32')\n",
    "        if np.max(dataset) > maxD:\n",
    "            maxD = np.max(dataset)\n",
    "        if np.min(dataset) < minD:\n",
    "            minD = np.min(dataset)\n",
    "        if (mode == 1):\n",
    "            sql = len(dataset)\n",
    "            dataX.append(dataset[:sql-1,0])\n",
    "            dataY.append(dataset[1:sql,0])\n",
    "        else:\n",
    "            for i in range(len(dataset)-look_back):\n",
    "                dataX.append(dataset[i:(i+look_back), 0])\n",
    "                dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY), minD, maxD\n",
    "\n",
    "# def create_dataset(ns, fname, look_back=1):\n",
    "#     dataX, dataY = [], []\n",
    "#     maxD = 0\n",
    "#     minD = 10000\n",
    "#     for c in ns:\n",
    "#         dataframe = pandas.read_csv(fname, usecols=[int(c)], engine='python')\n",
    "#         dataframe.dropna(how='all',inplace=True)\n",
    "#         dataset = dataframe.values\n",
    "#         dataset = dataset.astype('float32')\n",
    "#         if np.max(dataset) > maxD:\n",
    "#             maxD = np.max(dataset)\n",
    "#         if np.min(dataset) < minD:\n",
    "#             minD = np.min(dataset)\n",
    "#         sql = len(dataset)\n",
    "#         a = dataset[:sql-1,0]\n",
    "#         b = dataset[1:sql,0]\n",
    "#         dataX.append(a)\n",
    "#         dataY.append(b)\n",
    "#     return np.array(dataX), np.array(dataY), minD, maxD\n",
    "\n",
    "# # One step ahead prediction for sequnces of smaller lengths of size lookback\n",
    "# def create_dataset2(ns, fname, look_back=1):\n",
    "#     for c in ns:\n",
    "#         dataframe = pandas.read_csv(fname, usecols=[int(c)], engine='python')\n",
    "#         dataframe.dropna(how='all',inplace=True)\n",
    "#         dataset = dataframe.values\n",
    "#         dataset = dataset.astype('float32')\n",
    "#         if np.max(dataset) > maxD:\n",
    "#             maxD = np.max(dataset)\n",
    "#         if np.min(dataset) < minD:\n",
    "#             minD = np.min(dataset)\n",
    "#         for i in range(len(dataset)-look_back):\n",
    "#             a = dataset[i:(i+look_back), 0]\n",
    "#             dataX.append(a)\n",
    "#             dataY.append(dataset[i + look_back, 0])\n",
    "#     return np.array(dataX), np.array(dataY), minD, maxD\n",
    "\n",
    "# Input data sequence normalized over the population and the duration\n",
    "\n",
    "def create_dataset_2(mode, norm_flag, ns, fname, totalPop, ps, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for c in ns:\n",
    "        pop = totalPop[int(c)-1]\n",
    "        dataframe = pandas.read_csv(fname, usecols=[int(c)], engine='python')\n",
    "        dataframe.dropna(how='all',inplace=True)\n",
    "        dataset = dataframe.values\n",
    "        dataset = dataset.astype('float32')\n",
    "        #dataset = dataset/pop\n",
    "        #dataset = np.power(dataset,(1./ps))/40\n",
    "        if (mode == 1):\n",
    "            sql = len(dataset)\n",
    "            a = dataset[:sql-1,0]\n",
    "            b = dataset[1:sql,0]\n",
    "            if norm_flag == 1:\n",
    "                a = (np.sign(a)*np.abs(a)**(1./ps))/40\n",
    "                b = (np.sign(b)*np.abs(b)**(1./ps))/40\n",
    "            else:\n",
    "                a = (np.sign(a)*np.abs(a)**(1./ps))\n",
    "                b = (np.sign(b)*np.abs(b)**(1./ps))            \n",
    "        else:\n",
    "            for i in range(len(dataset)-look_back):\n",
    "                a = dataset[i:(i+look_back), 0]\n",
    "                b = dataset[i + look_back, 0]\n",
    "                if norm_flag == 1:\n",
    "                    a = (np.sign(a)*np.abs(a)**(1./ps))/40\n",
    "                    b = (np.sign(b)*np.abs(b)**(1./ps))/40\n",
    "                else:\n",
    "                    a = (np.sign(a)*np.abs(a)**(1./ps))\n",
    "                    b = (np.sign(b)*np.abs(b)**(1./ps))\n",
    "            \n",
    "        dataX.append(a)\n",
    "        dataY.append(b)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def create_dataset3(ns, fname, totalPop, ps, look_back=1):\n",
    "#     dataX, dataY = [], []\n",
    "#     for c in ns:\n",
    "#         pop = totalPop[int(c)-1]\n",
    "#         dataframe = pandas.read_csv(fname, usecols=[int(c)], engine='python')\n",
    "#         dataframe.dropna(how='all',inplace=True)\n",
    "#         dataset = dataframe.values\n",
    "#         dataset = dataset.astype('float32')\n",
    "#         #dataset = dataset/pop\n",
    "#         #dataset = np.power(dataset,(1./ps))/40\n",
    "#         sql = len(dataset)\n",
    "#         a = dataset[:sql-1,0]\n",
    "#         a = (np.sign(a)*np.abs(a)**(1./ps))/40\n",
    "#         b = dataset[1:sql,0]\n",
    "#         b = (np.sign(b)*np.abs(b)**(1./ps))/40\n",
    "#         dataX.append(a)\n",
    "#         dataY.append(b)\n",
    "#     return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# # Input data sequence normalized over the population\n",
    "# def create_dataset3p(ns, fname, totalPop, ps, look_back=1):\n",
    "#     dataX, dataY = [], []\n",
    "#     for c in ns:\n",
    "#         pop = totalPop[int(c)-1]\n",
    "#         dataframe = pandas.read_csv(fname, usecols=[int(c)], engine='python')\n",
    "#         dataframe.dropna(how='all',inplace=True)\n",
    "#         dataset = dataframe.values\n",
    "#         dataset = dataset.astype('float32')\n",
    "#         dataset = dataset/pop\n",
    "#         #dataset = np.power(dataset,(1./ps))/40\n",
    "#         sql = len(dataset)\n",
    "#         a = dataset[:sql-1,0]\n",
    "#         a = np.sign(a)*np.abs(a)**(1./ps)#/40\n",
    "#         b = dataset[1:sql,0]\n",
    "#         b = np.sign(b)*np.abs(b)**(1./ps)#/40\n",
    "#         dataX.append(a)\n",
    "#         dataY.append(b)\n",
    "#     return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# # Input data sequence of lookback length normalized over the population and the duration\n",
    "# def create_dataset4(ns, fname, totalPop, ps, look_back=1):\n",
    "#     dataX, dataY = [], []\n",
    "#     for c in ns:\n",
    "#         pop = totalPop[int(c)-1]\n",
    "#         dataframe = pandas.read_csv(fname, usecols=[int(c)], engine='python')\n",
    "#         dataframe.dropna(how='all',inplace=True)\n",
    "#         dataset = dataframe.values\n",
    "#         #dataset = dataset.astype('float32')\n",
    "#         #dataset = dataset/pop\n",
    "#         #dataset = np.power(dataset,(1./ps))\n",
    "#         for i in range(len(dataset)-look_back):\n",
    "#             a = dataset[i:(i+look_back), 0]\n",
    "#             #a = a/pop\n",
    "#             b = dataset[i + look_back, 0]\n",
    "#             a = (np.sign(a)*np.abs(a)**(1./ps))/40\n",
    "#             b = (np.sign(b)*np.abs(b)**(1./ps))/40\n",
    "#             dataX.append(a)\n",
    "#             dataY.append(b)\n",
    "#     return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# # Input data sequence of lookback length normalized over the population\n",
    "# def create_dataset5(ns, fname, totalPop, ps, look_back=1):\n",
    "#     dataX, dataY = [], []\n",
    "#     for c in ns:\n",
    "#         pop = totalPop[int(c)-1]\n",
    "#         dataframe = pandas.read_csv(fname, usecols=[int(c)], engine='python')\n",
    "#         dataframe.dropna(how='all',inplace=True)\n",
    "#         dataset = dataframe.values\n",
    "#         #dataset = dataset.astype('float32')\n",
    "#         dataset = dataset/pop\n",
    "#         #dataset = np.power(dataset,(1./ps))\n",
    "#         for i in range(len(dataset)-look_back):\n",
    "#             a = dataset[i:(i+look_back), 0]\n",
    "#             #a = a/pop\n",
    "#             b = dataset[i + look_back, 0]\n",
    "#             a = np.sign(a)*np.abs(a)**(1./ps)#/40\n",
    "#             b = np.sign(b)*np.abs(b)**(1./ps)#/40\n",
    "#             dataX.append(a)\n",
    "#             dataY.append(b)\n",
    "#     return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input columns from the csv file for the data\n",
    "Loc = np.asarray([1,2,3,4,5,7,8,9,11,12,15,16,17,18,19,20,21,22,23,24,25,26,27,28])\n",
    "\n",
    "# Other set of input combinations that can be used \n",
    "#Loc = np.asarray([1,2,3,4,5,7,8,9,11,12,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,31,32,33,37,39,40,42,44,45,46,47,48,49,50,51])\n",
    "#Loc = np.asarray([13,29,30,34,35,36,38,41,43])\n",
    "#Loc = np.asarray(np.linspace(1,51,51)) \n",
    "#Loc = np.asarray([1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-002a2bac9711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataframePop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SinglePop.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdatasetPop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframePop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdatasetPop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetPop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtotalPop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetPop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pandas' is not defined"
     ]
    }
   ],
   "source": [
    "# Applying the formatting to the training set\n",
    "\n",
    "dataframePop = pandas.read_csv('SinglePop.csv', engine='python')\n",
    "datasetPop = dataframePop.values\n",
    "datasetPop = datasetPop.astype('float32')\n",
    "totalPop = datasetPop[0,:]\n",
    "\n",
    "lb = 7\n",
    "ps = 3\n",
    "maxV = 40\n",
    "\n",
    "trainActive, targetActive = create_dataset_2( 0, 1, Loc,'Active_cases_data.csv', totalPop, ps, lb)\n",
    "\n",
    "trainTotal, targetTotal = create_dataset_2( 0, 1, Loc,'Total_cases_data.csv', totalPop, ps, lb)\n",
    "\n",
    "train1d, nv = create_dataset_2( 0, 1, Loc,'One-day-change_data.csv', totalPop, ps, lb)\n",
    "\n",
    "train2d, nv = create_dataset_2( 0, 1, Loc,'Three-day-change_data.csv', totalPop, ps, lb)\n",
    "\n",
    "train3d, nv = create_dataset_2( 0, 1, Loc,'Seven-day-change_data.csv', totalPop, ps, lb)\n",
    "\n",
    "# Minmax scaling for hospital density and population density\n",
    "trainPop, nv, minPop, maxPop = create_dataset( 0, Loc,'Pop_data.csv', lb)\n",
    "trainPop = (trainPop - minPop)/(maxPop - minPop)\n",
    "\n",
    "trainMed, nv, minMed, maxMed = create_dataset( 0, Loc,'Med_data.csv',lb)\n",
    "trainMed = (trainMed - minMed)/(maxMed - minMed)\n",
    "\n",
    "trainPop2, nv, minPop2, maxPop2 = create_dataset( 0, Loc-1,'TotalPop.csv',lb)\n",
    "trainPop2 = (trainPop2 - minPop2)/(maxPop2 - minPop2)\n",
    "\n",
    "\n",
    "# Concatenate data into final train data format\n",
    "numC = trainTotal.shape[0]\n",
    "numT = trainTotal.shape[1]\n",
    "trainX = np.concatenate((np.reshape(trainActive,(numC,numT,1)),np.reshape(trainTotal,(numC,numT,1)),np.reshape(train1d,(numC,numT,1)),np.reshape(train2d,(numC,numT,1)),np.reshape(train3d,(numC,numT,1)),np.reshape(trainPop,(numC,numT,1)),np.reshape(trainMed,(numC,numT,1)),np.reshape(trainPop2,(numC,numT,1))),axis=2)\n",
    "#trainX = np.concatenate((np.reshape(trainTotal,(sql,lb,1)),np.reshape(trainPop,(sql,lb,1)),np.reshape(trainMed,(sql,lb,1))),axis=2)\n",
    "\n",
    "trainY = np.concatenate((np.reshape(targetActive,(numC,1)),np.reshape(targetTotal,(numC,1))),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model Definition**\n",
    "\n",
    "Here, we define a custom LSTM architecture that integrates two temporal inputs and combines the two static attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the prediction arrays for different states\n",
    "numR = 1# Number of runs\n",
    "fcl = 180  # Number of days of prediction\n",
    "totalPredict = np.zeros((1,fcl))\n",
    "activePredict = np.zeros((1,fcl))\n",
    "\n",
    "# Texas\n",
    "totalPredictTx = np.zeros((1,fcl))\n",
    "activePredictTx = np.zeros((1,fcl))\n",
    "\n",
    "# Florida\n",
    "totalPredictFl = np.zeros((1,fcl))\n",
    "activePredictFl = np.zeros((1,fcl))\n",
    "\n",
    "# New York\n",
    "totalPredictNy = np.zeros((1,fcl))\n",
    "activePredictNy = np.zeros((1,fcl))\n",
    "\n",
    "\n",
    "for m_run in range(numR):\n",
    "    keras.backend.clear_session()\n",
    "    numT = 7   # number of timesteps  for look-back prediction\n",
    "    numF = 2   # number of input channels (model1)\n",
    "    numF2 = 8  # Batch size\n",
    "    \n",
    "    # Initialize the model \n",
    "    AllInput = Input(shape=(numT,numF2))\n",
    "    RecInput = Lambda(lambda AllInput:AllInput[:,:,:numF],name='RecIn')(AllInput)\n",
    "    HidInput = Lambda(lambda AllInput:AllInput[:,-1,5:7],name='HIn')(AllInput)\n",
    "    H = LSTM(50, input_shape=(numT,numF),return_sequences=False)(RecInput)\n",
    "    #DC = Dense(2,activation='relu')(H)\n",
    "    D2 = (Dense(50,activation='sigmoid'))(HidInput)\n",
    "    Data = Multiply()([H,D2])\n",
    "    #D3 = (Dense(500,activation='sigmoid'))(Data)\n",
    "    out = (Dense(2,activation='relu'))(Data)\n",
    "    CoVid = Model(AllInput,out)\n",
    "    #CoVid.summary()\n",
    "    \n",
    "    # The model is trained using mean squared error loss function and Adam Optimizer. Fit the model with the training data.\n",
    "    ADopt = optimizers.SGD(lr=0.1)\n",
    "    CoVid.compile(loss='mean_squared_error', optimizer=ADopt)\n",
    "    CoVid.fit(trainX, trainY, epochs=100, batch_size=40, verbose=1)\n",
    " \n",
    "\n",
    "    # Train the next model with a batch size of 8 .\n",
    "    AllInputI = Input(batch_shape=(1,None,numF2))\n",
    "    RecInputI = Lambda(lambda AllInputI:AllInputI[:,:,:numF],name='RecInI')(AllInputI)\n",
    "    HidInputI = Lambda(lambda AllInputI:AllInputI[:,-1,5:7],name='HInI')(AllInputI)\n",
    "    HI = LSTM(50, input_shape=(None,numF),return_sequences=False,stateful=False)(RecInputI)\n",
    "    #DCI = Dense(2,activation='relu')(HI)\n",
    "    D2I = Dense(50,activation='sigmoid')(HidInputI)\n",
    "    DataI = Multiply()([HI,D2I])\n",
    "    #D3I = (Dense(100,activation='sigmoid'))(DataI)\n",
    "    outI = Dense(2,activation='relu')(DataI)\n",
    "\n",
    "    CoVidI = Model(AllInputI,outI)\n",
    "    #CoVidI.summary()\n",
    "\n",
    "    # Transfer the pretrained weights to the new model.\n",
    "    WT = CoVid.get_weights()\n",
    "    CoVidI.set_weights(WT)\n",
    "    \n",
    "    # Set Bexar county data\n",
    "    BexarPop = 1975000\n",
    "    testPop =  1553\n",
    "\n",
    "    # Scale population density and hospital bed density\n",
    "    testMed =  (7839./BexarPop)*10000\n",
    "\n",
    "    testPop = (testPop - minPop)/(maxPop - minPop)\n",
    "\n",
    "    testMed = (testMed - minMed)/(maxMed - minMed)\n",
    "    \n",
    "    testPop2 = (BexarPop - minPop2)/(maxPop2-minPop2)\n",
    "\n",
    "    # Set the test set for Bexar county\n",
    "    testActive = np.asarray([29,29,39,45,56,68,81])\n",
    "    testTotal = np.asarray([29,29,39,45,57,69,84])\n",
    "    test1d = np.asarray([4,0,10,6,12,12,15])\n",
    "    test2d = np.asarray([29,29,14,16,28,30,39])\n",
    "    test3d = np.asarray([29,29,39,45,57,69,84])\n",
    "\n",
    "    # Normalize and reshape the test data\n",
    "    testActive = (np.sign(testActive) * np.abs(testActive)**(1./ps))/maxV\n",
    "    testTotal = (np.sign(testTotal) * np.abs(testTotal)**(1./ps))/maxV\n",
    "    test1d = (np.sign(test1d) * np.abs(test1d)**(1./ps))/maxV\n",
    "    test2d = (np.sign(test2d) * np.abs(test2d)**(1./ps))/maxV\n",
    "    test3d = (np.sign(test3d) * np.abs(test3d)**(1./ps))/maxV\n",
    "\n",
    "    f1 = np.reshape(testActive,(1,lb,1))\n",
    "    f2 = np.reshape(testTotal,(1,lb,1))\n",
    "    f3 = np.reshape(test1d,(1,lb,1))\n",
    "    f4 = np.reshape(test2d,(1,lb,1))\n",
    "    f5 = np.reshape(test3d,(1,lb,1))\n",
    "    f6 = np.reshape([testPop,testPop,testPop,testPop,testPop,testPop,testPop],(1,lb,1))\n",
    "    f7 = np.reshape([testMed,testMed,testMed,testMed,testMed,testMed,testMed],(1,lb,1))\n",
    "    f8 = np.reshape([testPop2,testPop2,testPop2,testPop2,testPop2,testPop2,testPop2],(1,lb,1))\n",
    "\n",
    "    totalPredictT = np.zeros((1,fcl))\n",
    "    activePredictT = np.zeros((1,fcl))\n",
    "\n",
    "    totalGT = np.asarray([25,29,29,39,45,57,69,84])\n",
    "    activeGT = np.asarray([25,29,29,39,45,56,68,81])\n",
    "\n",
    "    initX = np.concatenate((f1,f2,f3,f4,f5,f6,f7,f8),axis=2)\n",
    "    \n",
    "    gtl = 7\n",
    "    \n",
    "    # Looping through duration of forecast for step ahead predictions\n",
    "    for tp in range(7,fcl):\n",
    "        if tp < gtl:\n",
    "            singlePredict = CoVidI.predict(initX)\n",
    "            # Store the predictions\n",
    "            totalPredictT[0,tp] = np.power((np.float32(singlePredict[0][1]))*maxV,ps)\n",
    "            activePredictT[0,tp] = np.power((np.float32(singlePredict[0][0]))*maxV,ps)\n",
    "        else:\n",
    "            prevX = initX\n",
    "            oi = prevX[0,1:,:]\n",
    "            singlePredict = CoVidI.predict(initX)\n",
    "\n",
    "            prevT7 = np.power(prevX[0,0,1]*maxV,ps)\n",
    "\n",
    "            prevT3 = np.power(prevX[0,4,1]*maxV,ps)\n",
    "\n",
    "            prevT1 = np.power(prevX[0,-1,1]*maxV,ps)\n",
    "\n",
    "            currentT = np.power(np.float32(singlePredict[0][1])*maxV,ps)\n",
    "\n",
    "            f3 = np.max([0,(currentT - prevT1)])\n",
    "            f4 = np.max([0,(currentT - prevT3)])\n",
    "            f5 = np.max([0,(currentT - prevT7)])\n",
    "            # Normalize\n",
    "            f3 = (np.sign(f3) * np.abs(f3)**(1./ps))/maxV\n",
    "            f4 = (np.sign(f4) * np.abs(f4)**(1./ps))/maxV\n",
    "            f5 = (np.sign(f5) * np.abs(f5)**(1./ps))/maxV\n",
    "\n",
    "            ci = np.reshape(prevX[0,-1,5:],(1,3))\n",
    "            di = np.reshape(np.asarray([f3,f4,f5]),(1,3))\n",
    "            completeIn = np.concatenate((singlePredict,di,ci),axis=1)\n",
    "            initX = np.concatenate((oi,completeIn),axis=0)\n",
    "            initX = np.reshape(initX,(1,lb,numF2))\n",
    "            # Store the predictions\n",
    "            totalPredictT[0,tp] = np.power((np.float32(singlePredict[0][1]))*maxV,ps)\n",
    "            activePredictT[0,tp] = np.power((np.float32(singlePredict[0][0]))*maxV,ps)\n",
    "            \n",
    "    totalPredict = totalPredict + totalPredictT/numR\n",
    "    activePredict = activePredict + activePredictT/numR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
