{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# System libraries\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Library for reading through pdf\n",
    "# pip install pymupdf\n",
    "# pip install fitz\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection**  \n",
    "Using a bash script to save all the PDF files from the [google mobility site](https://www.google.com/covid19/mobility/) and store them in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p mobility_pdfs/\n",
    "cd mobility_pdfs/\n",
    "# hacky way to create a list of states in USA\n",
    "states_list=\"Alabama Alaska Arizona Arkansas California Colorado Connecticut Delaware Florida Georgia Hawaii Idaho Illinois Indiana Iowa Kansas Kentucky Louisiana Maine Maryland Massachusetts Michigan Minnesota Mississippi Missouri Montana Nebraska Nevada New_Hampshire New_Jersey New_Mexico New_York North_Carolina North_Dakota Ohio Oklahoma Oregon Pennsylvania Rhode_Island South_Carolina South_Dakota Tennessee Texas Utah Vermont Virginia Washington West_Virginia Wisconsin Wyoming\"\n",
    "country_list = \"US Spain Italy France Germany\"\n",
    "country_list_short = \"US ES IT FR DE\"\n",
    "date=\"2020-04-05\"\n",
    "\n",
    "# Get the pdfs for the mobility information of the states\n",
    "for state in $states_list ; do\n",
    "    curl -s -O https://www.gstatic.com/covid19/mobility/${date}_US_${state}_Mobility_Report_en.pdf\n",
    "done\n",
    "# Get the pdf for the US country\n",
    "curl -s -o ${date}_US_US_Mobility_Report_en.pdf https://www.gstatic.com/covid19/mobility/${date}_US_Mobility_Report_en.pdf\n",
    "\n",
    "for country in $country_list_short ; do\n",
    "    curl -s -O https://www.gstatic.com/covid19/mobility/${date}_${country}_Mobility_Report_en.pdf\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PDF Parser**  \n",
    "Obtained this script online for parsing from an incoming stream of data and reading the plots in the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_streaming_data(stream):\n",
    "    data_raw = []\n",
    "    data_transformed = []\n",
    "    rotparams = None\n",
    "    npatches = 0\n",
    "    for line in stream.splitlines():\n",
    "        if line.endswith(\" cm\"):\n",
    "            # page 146 of https://www.adobe.com/content/dam/acom/en/devnet/pdf/pdfs/pdf_reference_archives/PDFReference.pdf\n",
    "            rotparams = list(map(float,line.split()[:-1]))\n",
    "        elif line.endswith(\" l\"):\n",
    "            x,y = list(map(float,line.split()[:2]))\n",
    "            a,b,c,d,e,f = rotparams\n",
    "            xp = a*x+c*y+e\n",
    "            yp = b*x+d*y+f\n",
    "            data_transformed.append([xp,yp])\n",
    "            data_raw.append([x,y])\n",
    "        elif line.endswith(\" m\"):\n",
    "            npatches += 1\n",
    "        else:\n",
    "            pass\n",
    "    data_raw = np.array(data_raw)\n",
    "    basex, basey = data_raw[-1]\n",
    "    good = False\n",
    "    if basex == 0.:\n",
    "        data_raw[:,1] = basey - data_raw[:,1]\n",
    "        data_raw[:,1] *= 100/60.\n",
    "        data_raw = data_raw[data_raw[:,1]!=0.]\n",
    "        if npatches == 1: good = True\n",
    "    return dict(data=np.array(data_raw), npatches=npatches, good=good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PDF Parser**  \n",
    "The method below parses the pdf by reading lines from it iteratviely and based on specified conditions, stores the information in arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_page(doc, ipage, verbose=False):\n",
    "    # Set of the categories required\n",
    "    categories_list = [\n",
    "        \"Retail & recreation\",\n",
    "        \"Grocery & pharmacy\",\n",
    "        \"Parks\",\n",
    "        \"Transit stations\",\n",
    "        \"Workplace\",\n",
    "        \"Residential\",\n",
    "    ]\n",
    "    \n",
    "    counties = []\n",
    "    curr_county = None\n",
    "    curr_category = None\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "    pagetext = doc.getPageText(ipage)\n",
    "    lines = pagetext.splitlines()\n",
    "    tickdates = list(filter(lambda x:len(x.split())==3, set(lines[-10:])))\n",
    "    #print (tickdates)\n",
    "    count  =0\n",
    "    for line in lines:\n",
    "        # Removing unwanted data from the page\n",
    "        if (\"* Not enough data\") in line: continue\n",
    "        if (\"needs a significant volume of data\") in line: continue\n",
    "\n",
    "        # If found the category line, add it to the dictionary, else keep iterating over.\n",
    "        if any(line.startswith(category) for category in categories_list):\n",
    "            curr_category = line\n",
    "        elif curr_category:\n",
    "            data[curr_county][curr_category].append(line)\n",
    "\n",
    "        # Filtering data to find the county information\n",
    "        if (all(category not in line for category in categories_list)\n",
    "            and (\"compared to baseline\" not in line)\n",
    "            and (\"Not enough data\" not in line)\n",
    "           ):\n",
    "            # Only two counties per page\n",
    "            if len(data.keys()) == 2: break\n",
    "            count +=1\n",
    "            #print (line, count)    \n",
    "            counties.append(line)\n",
    "            curr_county = line\n",
    "            \n",
    "    # Debugging entry skipping \n",
    "    if (ipage==5):\n",
    "        print (ipage,counties, \"\\n\")\n",
    "    for county in data :\n",
    "        print (county)\n",
    "        newdata = {}\n",
    "    for county in data:\n",
    "        newdata[county] = {}\n",
    "        \n",
    "        for category in data[county]:\n",
    "            # Skipping the ones with no data. We get to know that based on the space and * in the Pdf\n",
    "            if category.endswith(\" \"): continue\n",
    "            temp = [x for x in data[county][category] if \"compared to baseline\" in x]\n",
    "            if not temp: continue\n",
    "            percent = int(temp[0].split()[0].replace(\"%\",\"\"))\n",
    "            newdata[county][category.strip()] = percent\n",
    "    data = newdata\n",
    "    for county in data :\n",
    "        print (county, data[county])\n",
    "    tomatch = []\n",
    "    #Create a list of counties and the available categories for the given county\n",
    "    for county in counties:\n",
    "        for category in categories_list:\n",
    "            if category in data[county]:\n",
    "                tomatch.append([county,category,data[county][category]])\n",
    "                \n",
    "\n",
    "    print(len(tomatch))\n",
    "    print(data)\n",
    "\n",
    "    # Get the readable plots from the page ( Since there are broken and empty plots in the page)\n",
    "    readableplots = []\n",
    "    xrefs = sorted(doc.getPageXObjectList(ipage), key=lambda x:int(x[1].replace(\"X\",\"\")))\n",
    "    for i,xref in enumerate(xrefs):\n",
    "        stream = doc.xrefStream(xref[0]).decode()\n",
    "        info = parse_streaming_data(stream)\n",
    "        if not info[\"good\"]: continue\n",
    "        readableplots.append(info)\n",
    "    \n",
    "    print(len(readableplots))\n",
    "    \n",
    "    ret = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for m,g in zip(tomatch,readableplots):\n",
    "        xs = g[\"data\"][:,0]\n",
    "        ys = g[\"data\"][:,1]\n",
    "        maxys = ys[np.where(xs==xs.max())[0]]\n",
    "        maxy = maxys[np.argmax(np.abs(maxys))]\n",
    "        \n",
    "        \n",
    "        # Parse the dates as text and then based on min to max value, create a range of dates and store it in the dictionary\n",
    "        ts = list(map(lambda x: pd.Timestamp(x.split(None,1)[-1] + \", 2020\"), tickdates))\n",
    "        low, high = min(ts), max(ts)\n",
    "        dr = list(map(lambda x:str(x).split()[0], pd.date_range(low, high, freq=\"D\")))\n",
    "        lutpairs = list(zip(np.linspace(0,200,len(dr)),dr))\n",
    "\n",
    "        dates = []\n",
    "        values = []\n",
    "        asort = xs.argsort()\n",
    "        xs = xs[asort]\n",
    "        ys = ys[asort]\n",
    "        for x,y in zip(xs,ys):\n",
    "            date = min(lutpairs, key=lambda v:abs(v[0]-x))[1]\n",
    "            dates.append(date)\n",
    "            values.append(round(y,3))\n",
    "\n",
    "        ret.append(dict(\n",
    "            county=m[0],category=m[1],change=m[2],\n",
    "            values=values,\n",
    "            dates=dates,\n",
    "            changecalc=maxy,\n",
    "        ))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the dataframe for the county and state data.\n",
    "def parse_state(state):\n",
    "    doc = fitz.Document(f\"pdfs/2020-04-05_US_{state}_Mobility_Report_en.pdf\")\n",
    "    data = []\n",
    "    # 2 because we are skipping the first 2 pages from the PDF.\n",
    "    for i in range(2,doc.pageCount-1):\n",
    "        #print (i, \"The actual page\")\n",
    "        for entry in parse_page(doc, i):\n",
    "            entry[\"state\"] = state\n",
    "            entry[\"page\"] = i\n",
    "            print (i, \"the page numbers\")\n",
    "            data.append(entry)\n",
    "    outname = f\"data/{state}.json.gz\"\n",
    "    df = pd.DataFrame(data)\n",
    "    #ncounties = df['county'].nunique()\n",
    "    ncounties =df['county'].nunique()\n",
    "    print(f\"Parsed {len(df)} plots for {ncounties} counties in {state}\")\n",
    "    df = df[[\"state\",\"county\",\"category\",\"change\",\"changecalc\",\"dates\", \"values\",\"page\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the dataframe for the specific state\n",
    "string = \"Texas\"\n",
    "df = parse_state(string)\n",
    "csv_name = string +\"_mobility.csv\"\n",
    "df.to_csv(csv_name)\n",
    "parse_state(string).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Script for scraping the initial two pages of the pdf file\n",
    "def parse_page_total(doc, ipage, verbose=False):\n",
    "    \"\"\"\n",
    "    First two pages\n",
    "    \"\"\"\n",
    "    category_list = [\n",
    "        \"Retail & recreation\",\n",
    "        \"Grocery & pharmacy\",\n",
    "        \"Parks\",\n",
    "        \"Transit stations\",\n",
    "        \"Workplaces\",  # They have workplaces there instead of workplace\n",
    "        \"Residential\",\n",
    "    ]\n",
    "\n",
    "    curr_category = None\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "    pagetext = doc.getPageText(ipage)\n",
    "    lines = pagetext.splitlines()\n",
    "    tickdates = []\n",
    "    for line in lines:\n",
    "\n",
    "        if (\"* Not enough data\") in line: continue\n",
    "        if (\"needs a significant volume of data\") in line: continue\n",
    "        # Added this condition to check for the extra text in the right of the page\n",
    "        if 'Mobility trends ' in line or 'hubs' in line: continue\n",
    "        # If found the category line, add it to the dictionary, else keep iterating over.\n",
    "        if any(line.startswith(category) for category in categoriy_list):\n",
    "            curr_category = line\n",
    "        # Checking for x axis in the details\n",
    "        elif line[:3] in ('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'):\n",
    "            tickdates.append(line)\n",
    "        elif line[0] not in ('+', '-'):\n",
    "            continue\n",
    "        elif curr_category:\n",
    "            data[curr_category] = data.get(curr_category, []) + [line]\n",
    "\n",
    "    newdata = {}\n",
    "    for category in data:\n",
    "        # Skipping the ones with no data. We get to know that based on the space and * in the Pdf\n",
    "        if category.endswith(\" \"): continue\n",
    "        temp = data[category][0]\n",
    "        percent = int(temp.split()[0].replace(\"%\",\"\"))\n",
    "        newdata[category.strip()] = percent\n",
    "    data = newdata\n",
    "\n",
    "    tomatch = []\n",
    "    # Create a list of counties and the available categories for the given county\n",
    "    for category in categories:\n",
    "        if category in data:\n",
    "            tomatch.append([category,data[category]])\n",
    "\n",
    "    # Get the readable plots from the page ( Since there are broken and empty plots in the page)\n",
    "    readableplots = []\n",
    "    xrefs = sorted(doc.getPageXObjectList(ipage), key=lambda x:int(x[1].replace(\"X\",\"\")))\n",
    "    for _, xref in enumerate(xrefs):\n",
    "        stream = doc.xrefStream(xref[0]).decode()\n",
    "        info = parse_streaming_data(stream)\n",
    "        if not info[\"good\"]:\n",
    "            continue\n",
    "        readableplots.append(info)\n",
    "    \n",
    "    print(len(readableplots))\n",
    "    \n",
    "    ret = []\n",
    "    \n",
    "    if len(tomatch) != len(readableplots):\n",
    "        return ret\n",
    "    \n",
    "    for m,g in zip(tomatch,plots):\n",
    "        xs = g[\"data\"][:,0]\n",
    "        ys = g[\"data\"][:,1]\n",
    "        maxys = ys[np.where(xs==xs.max())[0]]\n",
    "        maxy = maxys[np.argmax(np.abs(maxys))]\n",
    "        \n",
    "        \n",
    "        # Parse the dates as text and then based on min to max value, create a range of dates and store it in the dictionary\n",
    "        ts = list(map(lambda x: pd.Timestamp(x.split(None,1)[-1] + \", 2020\"), tickdates))\n",
    "        low, high = min(ts), max(ts)\n",
    "        dr = list(map(lambda x:str(x).split()[0], pd.date_range(low, high, freq=\"D\")))\n",
    "        lutpairs = list(zip(np.linspace(0,200,len(dr)),dr))\n",
    "\n",
    "        dates = []\n",
    "        values = []\n",
    "        asort = xs.argsort()\n",
    "        xs = xs[asort]\n",
    "        ys = ys[asort]\n",
    "        for x,y in zip(xs,ys):\n",
    "            date = min(lutpairs, key=lambda v:abs(v[0]-x))[1]\n",
    "            dates.append(date)\n",
    "            values.append(round(y,3))\n",
    "\n",
    "        ret.append(dict(\n",
    "            category=m[0],change=m[1],\n",
    "            values=values,\n",
    "            dates=dates,\n",
    "            changecalc=maxy,\n",
    "        ))\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Create the dataframe for the country data.\n",
    "\n",
    "def parse_country(country):\n",
    "    doc = fitz.Document(f\"mobility_pdfs/2020-04-05_{country}_Mobility_Report_en.pdf\")\n",
    "    data = []\n",
    "    for i in range(2):\n",
    "        for entry in parse_page_total(doc, i):\n",
    "            entry['state'] = state\n",
    "            entry['page'] = i\n",
    "            entry['county'] = 'total'\n",
    "            data.append(entry)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Will replace this with a loop once a list for the codes for all the other countries\n",
    "df_ES = parse_state_total(\"ES\")\n",
    "df_IT = parse_state_total(\"IT\")\n",
    "df_FR = parse_state_total(\"FR\")\n",
    "df_DE = parse_state_total(\"DE\")\n",
    "\n",
    "combined = [df_US, df_ES, df_IT, df_FR, df_DE]\n",
    "df_comb = pd.concat(combined)\n",
    "df_comb.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
